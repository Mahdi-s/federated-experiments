{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03dccfbc-12e0-4ee4-a5c0-2e6b8716c00e",
   "metadata": {},
   "source": [
    "#### Central Training Notebook without differential Privacy\r\n",
    "This notebook helps you set up a central machine learning workflow involving a Neural Network based on UNET architecture. It consists o f  sections starting from loading necessary external modules, i.e., pytorch, albumentations, and wandb, to name a few. Next, we'll move to loading our data, modifying it to ensure consistency, and finally defining our model and training. The last section involves examining the results of training, to ensure the validity of the model and ensuring privacy is preserved since we intend to use the model on sensitive medical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa445ed0-407a-4159-bfcc-010a99fcb274",
   "metadata": {},
   "source": [
    "### 1. Loading External Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3fc1a-cb7d-45a2-9b11-76b58dfd5e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a5900-22ff-454b-a490-a1bbf1e4354d",
   "metadata": {},
   "source": [
    "### 2. Custom Dataloader definition\n",
    "In this section we specify the path to our data on disk and load the images and associated masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90a09a-7def-4111-9caf-77ce17502782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WoundDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, num_images=None, start_index=0):\n",
    "        \"\"\"\n",
    "        :param image_dir: str, path to the directory containing the images\n",
    "        :param mask_dir: str, path to the directory containing the masks\n",
    "        :param transform: callable, optional transform to be applied on a sample\n",
    "        :param num_images: int, optional number of images to load\n",
    "        :param start_index: int, index to start loading images from\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.images.sort()  # Make sure images are sorted to have consistent order\n",
    "        \n",
    "        if num_images is not None:\n",
    "            end_index = start_index + num_images\n",
    "            self.images = self.images[start_index:end_index]\n",
    "        else:\n",
    "            self.images = self.images[start_index:]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff6beda-f600-4b68-99e1-87bd0db48c1c",
   "metadata": {},
   "source": [
    "### 3. UNET Model Definition\n",
    "\n",
    "The provided class defines a deep learning architecture called \"UNET,\" which is a convolutional neural network (CNN) commonly used for semantic image segmentation tasks. UNET is known for its symmetric architecture with a contracting path (encoder) and an expansive path (decoder). The code is structured into two main classes: DoubleConv and UNET.\n",
    "\n",
    "The DoubleConv class represents a basic building block within the UNET architecture. It consists of two consecutive convolutional layers, each followed by batch normalization and a rectified linear unit (ReLU) activation function. This sequence of operations is designed to capture and enhance important image features while maintaining spatial information. The class is parameterized by the number of input channels (in_channels) and the number of output channels (out_channels) for the convolutional layers. These building blocks are used both in the contracting and expanding paths of the UNET architecture.\n",
    "\n",
    "The UNET class defines the UNET architecture itself. It takes several arguments, including the number of input channels (in_channels), the number of output channels (out_channels), and a list of feature map sizes (features) that determine the depth of the network. The UNET architecture consists of a downsampling (encoder) path, a bottleneck layer, and an upsampling (decoder) path. The downsampling path repeatedly applies the DoubleConv blocks while reducing spatial dimensions through max-pooling operations. The bottleneck layer further captures abstract features. The upsampling path includes transposed convolutional layers to upsample the feature maps and concatenate them with skip connections from the downsampling path. Finally, a 1x1 convolutional layer is applied to produce the segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c08ce-cf94-4ca3-abb3-55c46040b489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f0449-1877-485c-8e4f-48d4f16737b6",
   "metadata": {},
   "source": [
    "### 4. Utility Function Definitions\n",
    "\n",
    "save_checkpoint: This function is used to save the current state of the training process, allowing you to resume training at a later time or use the trained model for inference. The state of the model, along with other necessary parameters, is saved to a file specified by the filename parameter.\r\n",
    "\r\n",
    "state: This is a dictionary containing various elements of the model's state, such as the model parameters, optimizer state, and potentially other relevant information needed to resume training or perform inference.\r\n",
    "filename: The name of the file where the checkpoint will be saved.\r\n",
    "load_checkpoint: This function is used to load a saved checkpoint into a model, restoring its state to continue training or perform inference.\r\n",
    "\r\n",
    "checkpoint: A dictionary containing the saved state of the model.\r\n",
    "model: The neural network model into which the checkpoint will be loaded.\r\n",
    "get_loaders: This function is used to create and return the data loaders for the training and validation datasets.\r\n",
    "\r\n",
    "train_dir, train_maskdir: The directory paths for the training images and their corresponding masks.\r\n",
    "val_dir, val_maskdir: The directory paths for the validation images and their corresponding masks.\r\n",
    "batch_size: The number of samples per batch.\r\n",
    "train_transform, val_transform: Transformations to be applied to the training and validation datasets, respectively.\r\n",
    "num_workers: The number of worker threads to use for data loading.\r\n",
    "pin_memory: If set to True, this will copy Tensors into CUDA pinned memory before returning them, which can result in faster data transfer to CUDA-enabled GPUs.\r\n",
    "The function returns two data loaders: train_loader for the training dataset and val_loader for the validation dataset.\r\n",
    "save_predictions_as_imgs: This function is used to save the model's predictions on a dataset as images to a specified folder.\r\n",
    "\r\n",
    "loader: A DataLoader object providing batches of input data and corresponding ground truth labels.\r\n",
    "model: The neural network model whose predictions will be saved.\r\n",
    "folder: The directory path where the prediction images will be saved.\r\n",
    "device: The device on which the model and data should be loaded before making predictions. Typically this \n",
    "\n",
    "is a CUDA-enabled GPU.\r\n",
    "The model is set to evaluation mode before making predictions, ensuring that certain layers (such as dropout and batch normalization) behave differently during inference compared to training.\r\n",
    "The function saves two sets of images: the modelâ€™s predictions and the ground truth masks, allowing for easy comparison between the pred\n",
    "icted and actual values.\r\n",
    "Together, these functions provide utility for training, evaluating, and saving the results of a deep learning model, specifically for image segmentation tasks. The WoundDataset class mentioned in the functions seems to be a custom dataset class for handling images and their corresponding segmentation masks, although its implementation is not provided in the given code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf91cbc-7fe8-4c5f-a9a9-115757b9c94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint_large_04lr.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    train_ds = WoundDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "        num_images=10,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = WoundDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"central/adam/prediction_images/\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87847413-2d8a-4aff-a051-884efb305873",
   "metadata": {},
   "source": [
    "### 5. Train & Validation Function Definitions\n",
    "\r\n",
    "confusion_matrix(preds, y):\r\n",
    "This function calculates the confusion matrix values, including true positives (tp), false positives (fp), false negatives (fn), and true negatives (tn), given the model's predictions (preds) and ground truth labels (y). These values are essential for evaluating binary classification performance.\r\n",
    "\r\n",
    "validate_model(loader, model, device=\"cuda\"):\r\n",
    "This function evaluates the model's performance on a given data loader (loader) in validation mode. It calculates various metrics such as loss, accuracy, Dice score, IoU (Intersection over Union) score, and confusion matrix values for binary segmentation. It also tracks the total number of correct pixels and total pixels processed during evaluation.\r\n",
    "\r\n",
    "train_model(loader, model, optimizer, loss_fn, scaler):\r\n",
    "This function is responsible for training the model on a provided data loader (loader). It iterates through the data in batches, computes predictions, calculates the loss using a specified loss function (loss_fn), and updates the model's weights through backpropagation. It uses gradient scaling with torch.cuda.amp.autocast() to handle mixed-precision training when applicable. Additionally, it reduces the learning rate on a plateau using a learning rate scheduler (optim.lr_scheduler.ReduceLROnPlateau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f4a22-a5f6-4b14-8cfe-12861667251e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(preds, y):\n",
    "    tp = ((preds == 1) & (y == 1)).sum()\n",
    "    fp = ((preds == 1) & (y == 0)).sum()\n",
    "    fn = ((preds == 0) & (y == 1)).sum()\n",
    "    tn = ((preds == 0) & (y == 0)).sum()\n",
    "\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def validate_model(loader, model, criterion, device=\"cuda\"):\n",
    "    \"\"\"Test the network on the training set.\"\"\"\n",
    "    print(\"~~~~ In test ~~~~\")\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    iou_score  = 0\n",
    "    tn_sum = fp_sum = fn_sum = tp_sum = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            loss += criterion(preds, y).item()\n",
    "            preds = (preds > 0.5).float()\n",
    "            tn, fp, fn, tp = confusion_matrix(preds, y)\n",
    "            tn_sum += tn\n",
    "            fp_sum += fp\n",
    "            fn_sum += fn\n",
    "            tp_sum += tp\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds.sum() + y.sum()) + 1e-8\n",
    "            )\n",
    "            intersection = (preds * y).sum()\n",
    "            union = (preds + y).sum() - intersection\n",
    "            iou_score += (intersection + 1e-8) / (union + 1e-8)\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    loss /= num_batches\n",
    "    acc = num_correct/num_pixels*100\n",
    "    diceS = dice_score/num_batches\n",
    "    iouS = iou_score/num_batches\n",
    "    correct_pixel = num_correct\n",
    "    total_pixel = num_pixels\n",
    "    print(f\"Loss = {loss}\")\n",
    "    print(f\"IoU Score = {iouS}\")\n",
    "    print(f\"Dice Score = {diceS}\")\n",
    "    print(\"~~~~~ Out of test ~~~~~\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    result = [acc.item(), diceS.item(), iouS.item(), loss, correct_pixel.item(), total_pixel, tn_sum.item(), fp_sum.item(), fn_sum.item(), tp_sum.item()]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def train_model(loader, model, optimizer, loss_fn, scaler):\n",
    "    print(\"In Train\")\n",
    "    print(f'length of loader {len(loader)}')\n",
    "    model.train()\n",
    "    loop = tqdm(loader)\n",
    "    #scheduler =  optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update tqdm loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    #scheduler.step(loss)\n",
    "\n",
    "    print(\"Out of Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476b0ed-112b-4adf-88f4-c4fa9cb5585b",
   "metadata": {},
   "source": [
    "### 6. Define Hyperparameters and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5c93e-4763-4046-b325-f2f76e9deacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = {'epoch':[]\n",
    "            , 'accuracy':[]\n",
    "            , 'dice_score':[]\n",
    "            , 'IoU_score':[]\n",
    "            , 'Loss': []\n",
    "            , 'correct_pixels':[]\n",
    "            , 'total_pixel':[]\n",
    "            , 'tn': []\n",
    "            , 'fp': []\n",
    "            , 'fn': []\n",
    "            , 'tp': []\n",
    "            , 'lr': []}\n",
    "\n",
    "labels = [\"Accuracy\", \"Dice Score\", \"IOU\", \"Loss\", \"Correct Pixel\", \"Total Pixel\", \"tn\", \"fp\", \"fn\", \"tp\",\"lr\"]\n",
    "\n",
    "# run a loop to test various VS, LR, and Image sizes\n",
    "BATCH_SIZES = [25, 4, 6, 8, 10, 12, 15, 20, 25, 30]\n",
    "learning_rates = [0.00001, 0.00001, 0.000001]\n",
    "imageSize = [140, 240, 512]\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "# define address for your training and val data \n",
    "TRAIN_IMG_DIR = \"../../wound_data/data/woundData/train_images\"\n",
    "TRAIN_MASK_DIR = \"../../wound_data/data/woundData/train_masks\"\n",
    "VAL_IMG_DIR = \"../../wound_data/data/woundData/val_images\"\n",
    "VAL_MASK_DIR = \"../../wound_data/data/woundData/val_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f97eec-66ef-4ef5-9260-d21cff444805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fb764-5675-4dc0-9a34-f9c5202124bc",
   "metadata": {},
   "source": [
    "### 7. Set optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71961ed7-1a05-4be5-8b0b-6e66c6a768db",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_flag = \"adam\"\n",
    "#optim_flag = \"sgd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f07fd-14cc-4d50-ab44-8804a4811afa",
   "metadata": {},
   "source": [
    "### 8. Image tranform functions\n",
    "\n",
    "##### train_transform:\n",
    "\n",
    "This transformation pipeline is designed for augmenting and preprocessing images in the training dataset. It includes several operations:\n",
    "A.Resize: Resizes the input images to a specified height and width (IMAGE_HEIGHT and IMAGE_WIDTH), ensuring that all training images have consistent dimensions.\n",
    "A.Rotate: Randomly rotates the images within a specified limit (35 degrees in this case) to increase the diversity of training samples. This helps the model generalize better to different orientations of objects in the images.\n",
    "A.HorizontalFlip: Randomly flips images horizontally with a probability of 0.5. This operation simulates variations in object orientation, contributing to better model robustness.\n",
    "A.VerticalFlip: Randomly flips images vertically with a lower probability (0.1). It introduces additional variability by considering vertical reflections of objects.\n",
    "A.Normalize: Standardizes pixel values by subtracting the mean and dividing by the standard deviation. In this case, it centers the data around zero with unit variance. The max_pixel_value parameter ensures that pixel values are in the [0, 1] range.\n",
    "ToTensorV2: Converts the image and mask data into PyTorch tensors, making them compatible with the model's input format.\n",
    "\n",
    "##### val_transforms:\n",
    "\n",
    "This transformation pipeline is intended for preprocessing images in the validation dataset. It includes:\n",
    "A.Resize: Resizes the validation images to the same height and width as specified for the training images, ensuring consistent dimensions between the training and validation datasets.\n",
    "A.Normalize: Performs the same pixel value normalization as in the training transform. However, it doesn't include data augmentation operations like rotation or flips, as the goal of validation is to evaluate the model's performance on unaltered data.\n",
    "\n",
    "### 9. Training our Model\n",
    "\n",
    "Hyperparameter Exploration Loop: The code defines a loop that iterates over different batch sizes and image sizes. Inside this loop, it reconfigures the batch size and image dimensions according to the current iteration. This allows for a systematic exploration of how these hyperparameters affect model training and evaluation.\n",
    "\n",
    "Training and Evaluation: For each combination of batch size and image size, the code sets up data loaders, model, loss function, optimizer, and other training-related components. It then performs the following steps:\n",
    "\n",
    "Validates the model on the validation dataset to record initial performance metrics.\n",
    "Conducts training for a specified number of epochs, saving model checkpoints and recording training metrics (e.g., accuracy, dice score, IoU) for each epoch.\n",
    "Appends the results of each epoch to a DataFrame, allowing for easy tracking and analysis of training progress.\n",
    "Saves the DataFrame as a CSV file, preserving the training and evaluation results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc8119-d1f2-4949-b731-ba34a5207b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    for bs in BATCH_SIZES:\n",
    "        for img_size in imageSize:\n",
    "            \n",
    "            df = pd.DataFrame(columns=column_names)\n",
    "            LEARNING_RATE = lr\n",
    "            BATCH_SIZE = bs\n",
    "            NUM_EPOCHS = 200\n",
    "            NUM_WORKERS = 0\n",
    "            IMAGE_HEIGHT = img_size \n",
    "            IMAGE_WIDTH = img_size  \n",
    "        \n",
    "            csv_filename = str(optim_flag) + \"_output_\" + \"bs\" + str(BATCH_SIZE) + \"_lr\" + str(LEARNING_RATE) +\"_imgSize\"+ str(IMAGE_HEIGHT)+\"_epoch\"+ str(NUM_EPOCHS)+\".csv\"\n",
    "\n",
    "            train_transform = A.Compose(\n",
    "                [\n",
    "                    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "                    A.Rotate(limit=35, p=1.0),\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.VerticalFlip(p=0.1),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.0, 0.0, 0.0],\n",
    "                        std=[1.0, 1.0, 1.0],\n",
    "                        max_pixel_value=255.0,\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            # used to transform validation set\n",
    "            val_transforms = A.Compose(\n",
    "                [\n",
    "                    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.0, 0.0, 0.0],\n",
    "                        std=[1.0, 1.0, 1.0],\n",
    "                        max_pixel_value=255.0,\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            \n",
    "            train_loader, val_loader = get_loaders(\n",
    "                TRAIN_IMG_DIR,\n",
    "                TRAIN_MASK_DIR,\n",
    "                VAL_IMG_DIR,\n",
    "                VAL_MASK_DIR,\n",
    "                BATCH_SIZE,\n",
    "                train_transform,\n",
    "                val_transforms,\n",
    "                NUM_WORKERS,\n",
    "                PIN_MEMORY,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            model = UNET().to(DEVICE)\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "            if optim_flag == \"adam\":\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            else:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "            checkpoint_name = \"central/\"+str(optim_flag)+\"/checkpoint2/\"+str(optim_flag)+\"_checkpoint_imgSize\" + str(IMAGE_WIDTH) + \"_bs\" + str(BATCH_SIZE) + \"_lr\" + str(LEARNING_RATE) + \".pth.tar\"\n",
    "            print('check point')\n",
    "            print(checkpoint_name)\n",
    "            \n",
    "            if LOAD_MODEL:\n",
    "                load_checkpoint(torch.load(checkpoint_name), model)\n",
    "            \n",
    "            single_epoch_result = validate_model(val_loader, model, loss_fn, device=DEVICE)\n",
    "            print(f'length of train loader -> {len(train_loader)}')\n",
    "            print(\"First Check - Before training starts\")\n",
    "            for label, item in zip(labels, single_epoch_result):\n",
    "                print(f\"{label}: {item}\")\n",
    "                \n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "            \n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                single_epoch_result = []\n",
    "                train_model(train_loader, model, optimizer, loss_fn, scaler)\n",
    "                \n",
    "                # save model\n",
    "                checkpoint = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\":optimizer.state_dict(),\n",
    "                }\n",
    "                save_checkpoint(checkpoint, checkpoint_name)\n",
    "                print('checkpoint saved')\n",
    "                \n",
    "                # Saving model accuracy to a csv \n",
    "                #single_epoch_result.append(epoch)\n",
    "                single_epoch_result = validate_model(val_loader, model, loss_fn, device=DEVICE)\n",
    "                single_epoch_result.append(LEARNING_RATE)\n",
    "                \n",
    "                single_epoch_result.insert(0, epoch)\n",
    "                df.loc[len(df)] = single_epoch_result\n",
    "            \n",
    "                # print some examples to a folderthu\n",
    "                # save_predictions_as_imgs(\n",
    "                #     val_loader, model, folder=\"/central/adam/prediction_images/\", device=DEVICE\n",
    "                # )\n",
    "            \n",
    "            \n",
    "            # Save the DataFrame as a CSV file\n",
    "            csv_path = 'central/'+str(optim_flag)+'/dataframe_fix/' + csv_filename\n",
    "            df.to_csv(csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gerry3.9.17",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
