{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e39e694-214c-47f3-a974-2b94554a40b5",
   "metadata": {},
   "source": [
    "## Environment Setup and Library Imports\n",
    "\n",
    "In this section, we initialize the working environment and import necessary libraries for our federated learning project. Key components include:\n",
    "\n",
    "1. **PyTorch Libraries (`torch`, `torchvision`):** PyTorch is our primary framework for building and training neural network models. `torchvision` is used for image processing and transformation tasks.\n",
    "\n",
    "2. **Albumentations (`albumentations`):** A powerful library for image augmentation, allowing us to apply various transformations to our images, which is crucial for enhancing the robustness of our model.\n",
    "\n",
    "3. **Flower (`flwr`):** This is a federated learning framework that we use to simulate and manage federated learning environments. It helps in distributing the model training across multiple clients.\n",
    "\n",
    "4. **WandB (`wandb`):** Used for experiment tracking and visualization. It helps in monitoring the training process, logging metrics, and comparing different runs.\n",
    "\n",
    "5. **CUDA Environment:** We set up CUDA for GPU acceleration to speed up our training process. This is critical for handling large-scale data and complex model architectures efficiently.\n",
    "\n",
    "Understanding these components is essential for grasping the workflow and architecture of our federated learning experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612a946-47c5-444e-b0d5-bc95c3a1ba65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import albumentations as A\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import flwr as fl\n",
    "import torch.optim as optim\n",
    "from flwr.common import Metrics\n",
    "import wandb\n",
    "from typing import List, Union\n",
    "from flwr.common import Parameters, Scalar\n",
    "from flwr.server.client_proxy import ClientProxy, FitRes\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bae183-cecd-4c97-883e-6c99c8a95a28",
   "metadata": {},
   "source": [
    "## Configuration of Initial Parameters\n",
    "\n",
    "Before we dive into the data processing and model training, it's crucial to define some initial parameters that will be used throughout the notebook. These parameters are fundamental to our federated learning setup and data handling:\n",
    "\n",
    "1. **Number of Clients (`NUM_CLIENTS`):** This parameter specifies the number of clients that will participate in the federated learning process. In a federated learning environment, the model is trained across multiple decentralized devices or servers holding local data samples.\n",
    "\n",
    "2. **Batch Size (`BATCH_SIZE`):** The batch size determines how many samples will be processed before the model's internal parameters are updated. It is a key factor in optimizing the training process, balancing speed and resource consumption.\n",
    "\n",
    "3. **Image Dimensions (`IMAGE_HEIGHT`, `IMAGE_WIDTH`):** These parameters define the height and width of the images that will be used for training. Resizing images to uniform dimensions is important for consistent processing and is often a requirement for neural network inputs.\n",
    "\n",
    "Setting these parameters correctly is essential for ensuring that the federated learning process runs smoothly and that the data is handled properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a236d-8232-4e32-9a5b-69214ed182c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3\n",
    "BATCH_SIZE = 12\n",
    "IMAGE_HEIGHT = 240\n",
    "IMAGE_WIDTH = 240"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac733dea-772f-4648-91c2-5337a66f3a6a",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Transformation\n",
    "\n",
    "Data preprocessing and transformation are critical steps in preparing our dataset for effective model training, especially in image-based applications. In this section, we define the transformations that will be applied to our images:\n",
    "\n",
    "1. **Image Resizing:** We resize images to the predefined dimensions (`IMAGE_HEIGHT` and `IMAGE_WIDTH`). This ensures uniformity in input size for the model.\n",
    "\n",
    "2. **Augmentation Techniques:**\n",
    "   - **Rotation (`Rotate`):** Images are rotated by a specified limit to introduce variance and make the model robust to orientation changes.\n",
    "   - **Horizontal Flip (`HorizontalFlip`):** This augmentation flips the image horizontally with a certain probability, further diversifying the dataset.\n",
    "   - **Vertical Flip (`VerticalFlip`):** Similar to horizontal flipping, but in the vertical direction, introducing more variance.\n",
    "\n",
    "3. **Normalization (`Normalize`):** We normalize the image pixel values. Normalization is a common practice in image processing that makes model training more efficient and stable.\n",
    "\n",
    "4. **Conversion to Tensor (`ToTensorV2`):** The transformed image is converted into a PyTorch tensor, which is the required format for training neural network models in PyTorch.\n",
    "\n",
    "These transformations are packaged into a `transform` function, which will be applied to our training dataset. We also define `val_transforms` for the validation dataset, which includes resizing and normalization, but without the additional augmentation techniques used in the training dataset. This distinction is important to evaluate the model's performance on more standard data forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81ede7-3dd5-4c07-858e-b6a9d620de12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the transform\n",
    "transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# used to transform validation set\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780b983-4650-4789-bd36-ad5dbba95bdd",
   "metadata": {},
   "source": [
    "## Custom Function for Dataset Splitting\n",
    "\n",
    "This section introduces a custom function named `adjusted_random_split`, designed to split a dataset into subsets of specified lengths. This function is particularly useful in scenarios where the standard random split might not perfectly divide the dataset due to rounding issues or specific length requirements.\n",
    "\n",
    "### Functionality of `adjusted_random_split`:\n",
    "1. **Handling Standard Split:** Initially, the function attempts to split the dataset into subsets based on the provided `lengths`. This is done using PyTorch's `random_split` function.\n",
    "\n",
    "2. **Error Handling and Adjustment:**\n",
    "   - The function includes error handling to catch a `ValueError` that occurs if the sum of the specified lengths does not match the total length of the dataset.\n",
    "   - If this error is encountered, the function adjusts the length of the last subset to compensate for any discrepancy. This ensures that the total length of all subsets exactly matches the length of the dataset.\n",
    "\n",
    "3. **Retrying the Split:** After adjusting the lengths, the function retries the split operation to ensure successful partitioning of the dataset.\n",
    "\n",
    "This custom function is essential for ensuring precise dataset splitting, especially in a federated learning context where data distribution among different clients needs to be controlled and accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399953a6-64d8-4d32-b219-a8ee9ec08a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_random_split(dataset, lengths):\n",
    "    try:\n",
    "        subsets = random_split(dataset, lengths)\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Sum of input lengths does not equal the length of the input dataset!\":\n",
    "            print(\"Length mismatch detected. Adjusting lengths to match dataset.\")\n",
    "            lengths[-1] += len(dataset) - sum(lengths)  # adjust last split length to match dataset length\n",
    "            subsets = random_split(dataset, lengths)  # retry the split\n",
    "        else:\n",
    "            raise  # re-raise the exception if it's not what we're expecting\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0305e45-0618-47f4-9a70-3c5b954d7292",
   "metadata": {},
   "source": [
    "## Custom Dataset Class for Wound Images\n",
    "\n",
    "This section of the notebook defines a custom dataset class named `WoundDataset`, which is a subclass of PyTorch's `Dataset`. This class is tailored for handling a dataset consisting of wound images and their corresponding masks, crucial for tasks like image segmentation in medical imaging.\n",
    "\n",
    "### Structure and Functionality of `WoundDataset`:\n",
    "1. **Initialization (`__init__`):**\n",
    "   - The constructor takes `image_dir` (directory containing images), `mask_dir` (directory containing corresponding masks), and an optional `transform`.\n",
    "   - It initializes the dataset by listing all images in `image_dir`.\n",
    "\n",
    "2. **Length Method (`__len__`):**\n",
    "   - This method returns the total number of images in the dataset, allowing PyTorch to understand the dataset's size.\n",
    "\n",
    "3. **Item Access Method (`__getitem__`):**\n",
    "   - Given an index, this method loads the image and its corresponding mask from the directories.\n",
    "   - The images are converted to the RGB format, and masks are converted to grayscale. The mask values are also adjusted to ensure a binary format (0 for background, 1 for wound).\n",
    "   - If transformations are specified, they are applied to both the image and the mask. This is crucial for data augmentation and preprocessing.\n",
    "   - The method returns a tuple containing the transformed image and mask.\n",
    "\n",
    "This custom class is essential for efficiently loading and preprocessing the dataset, particularly for a task that involves simultaneous handling of images and their corresponding segmentation masks. By customizing the dataset loading process, we can ensure that the data is correctly formatted and transformed, ready for use in model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72589518-d1a3-4f0a-adf1-5b257b851c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WoundDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "        return image, mask #, self.images[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17425a-2135-4aa4-95bd-af8de44fd64a",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Distribution for Federated Learning\n",
    "\n",
    "This section of the notebook focuses on loading the wound images and masks, combining them into a single dataset, and then distributing them among clients for federated learning. This process is critical for ensuring that each client receives a representative portion of the data.\n",
    "\n",
    "### Steps Involved in Dataset Preparation:\n",
    "1. **Loading Datasets:**\n",
    "   - We use the `WoundDataset` class to load training and testing datasets separately for images and masks.\n",
    "\n",
    "2. **Combining Datasets:**\n",
    "   - The training and testing datasets are combined into one dataset using `ConcatDataset`. This is done to shuffle and redistribute them into new training and testing sets.\n",
    "\n",
    "3. **Shuffling the Combined Dataset:**\n",
    "   - The combined dataset is shuffled to ensure randomness and prevent any bias that might be present in the way the data was originally ordered.\n",
    "\n",
    "4. **Splitting into New Training and Testing Sets:**\n",
    "   - The shuffled dataset is then split into new training and testing sets. The split ratio is determined randomly between 70% and 90% to vary the size of the training set.\n",
    "\n",
    "5. **Adjusted Random Split for Federated Learning:**\n",
    "   - To ensure each client receives at least one image, we distribute the datasets among clients. We start by assigning one image to each client and then distribute the remaining images randomly.\n",
    "   - The `adjusted_random_split` function is used to handle the splitting, ensuring that the sum of splits matches the dataset size exactly.\n",
    "\n",
    "6. **Creating Data Loaders:**\n",
    "   - For each subset of the new training and testing sets, a `DataLoader` is created. These loaders are essential for batch processing during model training and evaluation.\n",
    "\n",
    "7. **Diagnostic Print Statements:**\n",
    "   - Finally, print statements are used to output the size of the new train and test sets, the split ratio, and the number of batches per client. This helps in verifying the distribution and ensuring everything is as expected.\n",
    "\n",
    "Through these steps, we prepare our datasets in a manner that suits federated learning, where data is distributed across multiple clients for decentralized training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3529b3-dd4e-470c-ab5c-0067ea94fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and masks\n",
    "trainset = WoundDataset(\"../../wound_data/data/woundData/train_images\", \"../../wound_data/data/woundData/train_masks\", transform=transform)\n",
    "testset = WoundDataset(\"../../wound_data/data/woundData/val_images\", \"../../wound_data/data/woundData/val_masks\", transform=val_transforms)\n",
    "\n",
    "# Combine train and test sets\n",
    "combined_set = ConcatDataset([trainset, testset])\n",
    "\n",
    "#Shuffle the combined dataset\n",
    "indices = list(range(len(combined_set)))\n",
    "random.shuffle(indices)\n",
    "shuffled_set = torch.utils.data.Subset(combined_set, indices)\n",
    "\n",
    "# Split into new train and test \n",
    "split_ratio = random.uniform(0.7, 0.9)\n",
    "train_size = int(split_ratio * len(shuffled_set))\n",
    "test_size = len(shuffled_set) - train_size\n",
    "adj_trainset, adj_testset = random_split(shuffled_set, [train_size, test_size])\n",
    "\n",
    "# Ensure each client gets at least one picture\n",
    "lengths_train = [1] * NUM_CLIENTS\n",
    "lengths_test = [1] * NUM_CLIENTS\n",
    "\n",
    "# Distribute the remaining pictures randomly among the clients\n",
    "remaining_train_pics = len(adj_trainset) - NUM_CLIENTS\n",
    "remaining_test_pics = len(adj_testset) - NUM_CLIENTS\n",
    "\n",
    "for i in range(remaining_train_pics):\n",
    "    lengths_train[random.randint(0, NUM_CLIENTS - 1)] += 1\n",
    "\n",
    "for i in range(remaining_test_pics):\n",
    "    lengths_test[random.randint(0, NUM_CLIENTS - 1)] += 1\n",
    "\n",
    "new_trainset = adjusted_random_split(adj_trainset, lengths_train)\n",
    "new_testset = adjusted_random_split(adj_testset, lengths_test)\n",
    "\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "\n",
    "for trainset in new_trainset:\n",
    "    train_loaders.append(DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True))\n",
    "for testset in new_testset:\n",
    "    test_loaders.append(DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False))\n",
    "\n",
    "print(f\"New train set size: {len(adj_trainset)}\")\n",
    "print(f\"New test set size: {len(adj_testset)}\")\n",
    "print(f\"Split ratio: {split_ratio}\")\n",
    "print(type(train_loaders[0]))  \n",
    "\n",
    "for i, client_sample in train_loaders:\n",
    "    print(f\"Train Client {i} Size: {len(client_sample)}\")\n",
    "\n",
    "for i, client_sample in test_loaders:\n",
    "    print(f\"Test Client {i} Size: {len(client_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9fdc9-ddee-476a-9008-469038d95315",
   "metadata": {},
   "source": [
    "## Function to Display and Save Original Dataset Samples\n",
    "\n",
    "In this part of the notebook, we define a function named `display_original_samples`. This function is designed to display and save a specified number of original images and masks from the dataset. This visual inspection is crucial for understanding the nature of the data we are working with and for ensuring the quality of the images and masks before proceeding with training and analysis.\n",
    "\n",
    "### Overview of `display_original_samples` Function:\n",
    "1. **Function Parameters:**\n",
    "   - `dataset`: The dataset from which samples will be displayed.\n",
    "   - `num_samples`: The number of samples to display. By default, set to 3.\n",
    "   - `save_path`: The file path where the figure will be saved.\n",
    "   - `target_size`: The target size to which the images and masks will be resized.\n",
    "\n",
    "2. **Processing and Displaying Samples:**\n",
    "   - The function iterates through the specified number of samples from the dataset.\n",
    "   - Each image and its corresponding mask are resized to the `target_size` for consistent display.\n",
    "   - Both the original image and mask are displayed side by side for each sample. This helps in visualizing the data and understanding the correlation between the images and masks.\n",
    "\n",
    "3. **Customization and Saving the Figure:**\n",
    "   - Titles are added for clarity, and axes are turned off for a cleaner look, suitable for scientific presentations or publications.\n",
    "   - The layout is adjusted for optimal spacing, and the figure is saved to the specified `save_path`.\n",
    "   - The figure is also displayed inline in the notebook for immediate inspection.\n",
    "\n",
    "### Example Usage:\n",
    "After defining the function, an example usage is shown where the `WoundDataset` is loaded without transformations. The `display_original_samples` function is then called with this dataset to visualize the first three samples.\n",
    "\n",
    "This function is an essential tool for preliminary data analysis, allowing researchers to visually inspect the dataset and confirm that the images and masks are correctly aligned and processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bc7b8-eb38-4847-a332-fec26108e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_original_samples(dataset, num_samples=3, save_path='figures/original_samples.png', target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Display and save the first three original images and masks from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): The dataset to display images from.\n",
    "    num_samples (int): Number of samples to display, set to 3.\n",
    "    save_path (str): File path to save the figure.\n",
    "    target_size (tuple): The target size for images and masks.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, mask = dataset[i]\n",
    "\n",
    "        # Resize images and masks\n",
    "        resized_image = Image.fromarray(image).resize(target_size)\n",
    "        resized_mask = Image.fromarray(mask * 255).resize(target_size).convert(\"L\")\n",
    "\n",
    "        # Display images and masks\n",
    "        img_ax = ax[i, 0].imshow(np.array(resized_image))\n",
    "        mask_ax = ax[i, 1].imshow(np.array(resized_mask), cmap='gray')\n",
    "\n",
    "        # Customization for a scientific journal\n",
    "        ax[i, 0].set_title('Original Image')\n",
    "        ax[i, 1].set_title('Original Mask')\n",
    "        ax[i, 0].axis('off')\n",
    "        ax[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "original_dataset = WoundDataset(image_dir, mask_dir, transform=None)\n",
    "display_original_samples(original_dataset, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561510d-ae7d-43df-bb23-8c9e920533b0",
   "metadata": {},
   "source": [
    "## Function to Display and Save Samples from a Client's Dataset\n",
    "\n",
    "In this part of the notebook, we define a function named `display_client_samples`. This function is intended for displaying and saving a specified number of images and masks from a given client's dataset in a federated learning context. This is crucial for verifying the data each client receives and ensuring consistency and alignment of images and masks across different clients.\n",
    "\n",
    "### Overview of `display_client_samples` Function:\n",
    "1. **Function Parameters:**\n",
    "   - `data_loader`: The DataLoader associated with a specific client's dataset.\n",
    "   - `num_samples`: The number of samples to display, defaulting to 3.\n",
    "   - `save_path`: The file path where the figure will be saved.\n",
    "   - `target_size`: The target size for resizing images and masks.\n",
    "\n",
    "2. **Processing and Displaying Samples:**\n",
    "   - The function iterates through the DataLoader, processing and displaying only the specified number of samples.\n",
    "   - Each image and mask are converted from tensors to numpy arrays, scaled to a 0-255 range, and resized to the target size for consistent visualization.\n",
    "   - The images and masks are displayed side by side for each sample. This allows for a direct comparison and ensures that they are correctly aligned.\n",
    "\n",
    "3. **Customization and Saving the Figure:**\n",
    "   - Titles are added for clarity, and axes are turned off for a cleaner presentation.\n",
    "   - Colorbars are added next to each image and mask for better visual interpretation of the data.\n",
    "   - The layout is adjusted for optimal spacing, and the figure is saved to the specified `save_path`.\n",
    "   - The figure is also displayed in the notebook for immediate review.\n",
    "\n",
    "### Example Usage:\n",
    "The function is demonstrated with an example where it is called on a DataLoader from one of the clients (`train_loaders[client_id]`). This showcases how the function can be used to inspect the data distribution and quality for individual clients in a federated learning setup.\n",
    "\n",
    "This visualization function is an essential tool for analyzing the data distribution in federated learning scenarios, ensuring that each client receives appropriate and well-preprocessed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb990d9-34dd-47e1-8598-b2be145fe443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_client_samples(data_loader, num_samples=3, save_path='figures/sample_figure.png', target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Display and save the first three images and masks from a client's dataset, ensuring they are all the same size and aligned.\n",
    "\n",
    "    Parameters:\n",
    "    data_loader (DataLoader): The DataLoader for the client's dataset.\n",
    "    num_samples (int): Number of samples to display, set to 3.\n",
    "    save_path (str): File path to save the figure.\n",
    "    target_size (tuple): The target size for images and masks.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))  # Adjust layout for multiple samples\n",
    "\n",
    "    for i, (images, masks) in enumerate(data_loader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        # Process and resize images and masks\n",
    "        image = images[0].permute(1, 2, 0).numpy()  # Convert tensor to numpy array\n",
    "        image = (image * 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8\n",
    "        resized_image = Image.fromarray(image).resize(target_size)\n",
    "\n",
    "        mask = masks[0].numpy()  # Convert tensor to numpy array\n",
    "        mask = (mask * 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8\n",
    "        resized_mask = Image.fromarray(mask).resize(target_size)\n",
    "\n",
    "        # Display images and masks\n",
    "        img_ax = ax[i, 0].imshow(np.array(resized_image))\n",
    "        mask_ax = ax[i, 1].imshow(np.array(resized_mask), cmap='gray')\n",
    "\n",
    "        # Customization for a scientific journal\n",
    "        ax[i, 0].set_title('Original Image')\n",
    "        ax[i, 1].set_title('Mask')\n",
    "        ax[i, 0].axis('off')\n",
    "        ax[i, 1].axis('off')\n",
    "        \n",
    "        # Adding colorbars\n",
    "        fig.colorbar(img_ax, ax=ax[i, 0], orientation='vertical')\n",
    "        fig.colorbar(mask_ax, ax=ax[i, 1], orientation='vertical')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "client_id = 0  # Replace with the desired client ID\n",
    "display_client_samples(train_loaders[client_id], num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1453227-eb57-4799-9679-751ba5dd1862",
   "metadata": {},
   "source": [
    "## Defining the UNET Architecture for Image Segmentation\n",
    "\n",
    "In this section, we define the architecture of a U-Net, which is a convolutional neural network widely used for image segmentation tasks, particularly in medical imaging. The U-Net architecture is notable for its effectiveness in handling segmentation challenges where precise localization is crucial.\n",
    "\n",
    "### Structure of the U-Net Model:\n",
    "1. **Double Convolution Block (`DoubleConv`):**\n",
    "   - This module is a core component of the U-Net, performing two consecutive convolution operations.\n",
    "   - Each convolution is followed by batch normalization and a ReLU activation, ensuring non-linear transformations and stabilized learning.\n",
    "   - The `DoubleConv` block acts as a foundational element in both the downsampling (encoder) and upsampling (decoder) paths of the U-Net.\n",
    "\n",
    "2. **U-Net Architecture (`UNET`):**\n",
    "   - The U-Net model is characterized by its encoder-decoder structure.\n",
    "   - **Encoder:** The downsampling path consists of several `DoubleConv` blocks. The spatial dimensions of the input are reduced while increasing the feature depth, capturing complex features at different scales. Max pooling is used between these blocks for downsampling.\n",
    "   - **Bottleneck:** This part processes the output from the last downsampling step, serving as a bridge between the encoder and decoder paths.\n",
    "   - **Decoder:** The upsampling path mirrors the encoder, with transposed convolutions for upsampling the feature maps, followed by `DoubleConv` blocks. Skip connections from the corresponding encoder layers are concatenated with these upsampled outputs, aiding in precise localization and feature preservation.\n",
    "\n",
    "3. **Final Convolution:**\n",
    "   - A final convolutional layer at the end of the decoder path reduces the number of output channels to match the requirements of the specific segmentation task (e.g., binary mask for segmentation).\n",
    "\n",
    "### Practical Implementation:\n",
    "- The U-Net architecture is highly adaptable to various segmentation tasks. \n",
    "- Its design, featuring skip connections and deep feature extraction, is particularly effective in ensuring precise pixel-level segmentation, which is critical in fields like medical image analysis.\n",
    "\n",
    "The U-Net model outlined here provides a robust framework for tackling a wide range of image segmentation problems. Its effectiveness lies in its ability to capture both context and localization information, making it a popular choice in the field of medical image analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aaa984-2415-40f3-8a60-4c78635e8960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe307b-2eef-4fe3-b0f9-bdf4af77f097",
   "metadata": {},
   "source": [
    "## Functions for Model Evaluation and Training\n",
    "\n",
    "This section includes key functions essential for training and evaluating the segmentation model. It defines the `confusion_matrix` function for computing performance metrics, `check_accuracy` for evaluating model accuracy, and the `train` function for training the model.\n",
    "\n",
    "### Confusion Matrix Function (`confusion_matrix`):\n",
    "1. **Purpose:** Computes the confusion matrix for binary segmentation. This matrix is vital for understanding the model's performance in terms of true positives, true negatives, false positives, and false negatives.\n",
    "2. **Parameters:**\n",
    "   - `preds`: Predictions from the model, assumed to be binary values after thresholding.\n",
    "   - `y`: Ground truth labels.\n",
    "3. **Returns:** True negatives (tn), false positives (fp), false negatives (fn), and true positives (tp).\n",
    "\n",
    "### Accuracy Checking Function (`check_accuracy`):\n",
    "1. **Functionality:** Evaluates the model's performance on a given dataset (data loader). It uses metrics like loss, dice score, and IoU (Intersection over Union) score for a comprehensive evaluation.\n",
    "2. **Implementation Details:**\n",
    "   - The function calculates loss using binary cross-entropy with logits.\n",
    "   - It iterates over the dataset, computing the confusion matrix and other metrics for each batch.\n",
    "   - The function aggregates these metrics to provide an overall evaluation of the model's accuracy on the dataset.\n",
    "3. **Results:** Outputs include true negatives, false positives, false negatives, true positives, IoU score, dice score, and other relevant statistics.\n",
    "\n",
    "### Training Function (`train`):\n",
    "1. **Objective:** Manages the training process of the neural network on a specified training dataset.\n",
    "2. **Key Steps:**\n",
    "   - Sets up the loss function (binary cross-entropy with logits) and the optimizer (Adam).\n",
    "   - Optionally uses multiple GPUs for training if available.\n",
    "   - Employs gradient scaling to manage mixed precision training, enhancing performance.\n",
    "   - Iterates over the epochs, performing forward and backward passes for each batch.\n",
    "3. **Functionality:** This function is responsible for adjusting the model's weights based on the training data, with the aim of improving its segmentation accuracy.\n",
    "\n",
    "These functions are central to the model's lifecycle, from training to performance evaluation. They ensure the model is trained effectively and provide detailed insights into its accuracy and reliability in segmenting images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11df00-10c5-4856-b26f-165446889561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(preds, y):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix for binary segmentation.\n",
    "\n",
    "    Args:\n",
    "        preds: Predictions from the model. Assumes binary values after thresholding.\n",
    "        y: Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        tn: True negatives\n",
    "        fp: False positives\n",
    "        fn: False negatives\n",
    "        tp: True positives\n",
    "    \"\"\"\n",
    "    tp = (y * preds).sum().to(torch.float32)\n",
    "    tn = ((1 - y) * (1 - preds)).sum().to(torch.float32)\n",
    "    fp = ((1 - y) * preds).sum().to(torch.float32)\n",
    "    fn = (y * (1 - preds)).sum().to(torch.float32)\n",
    "\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def check_accuracy(loader, model, device=\"cuda\"):\n",
    "    \"\"\"Test the network on the training set.\"\"\"\n",
    "    print(\"~~~~ In test ~~~~\")\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    loss = 0    \n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    iou_score  = 0\n",
    "    result = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            loss += criterion(preds, y).item()\n",
    "            tn, fp, fn, tp = confusion_matrix(preds, y)\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds.sum() + y.sum()) + 1e-8\n",
    "            )\n",
    "            # Calculate IoU score\n",
    "            intersection = (preds * y).sum()\n",
    "            union = (preds + y).sum() - intersection\n",
    "            iou_score += (intersection + 1e-8) / (union + 1e-8)\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    loss /= num_batches\n",
    "    acc = num_correct/num_pixels*100\n",
    "    diceS = dice_score/num_batches\n",
    "    iouS = iou_score/num_batches\n",
    "    correct_pixel = num_correct\n",
    "    total_pixel = num_pixels\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"IoU Score = {iouS}\")\n",
    "    print(f\"Dice Score = {diceS}\")\n",
    "    print(\"~~~~~ Out of test ~~~~~\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    result = [acc.item(), diceS.item(), iouS.item(), loss, correct_pixel.item(), total_pixel]\n",
    "    \n",
    "    return result\n",
    "  \n",
    "\n",
    "def train(net, trainloader, epochs: int, lr, device=\"cuda\"):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    print(\"~~~~ In train ~~~~\")\n",
    "\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss() #loss_fn\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "    scaler = GradScaler() #torch.cuda.amp.\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(DEVICE)     \n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch => {epoch}')\n",
    "        print(f'length of training data {len(trainloader)}')\n",
    "        for images, labels in trainloader:\n",
    "            # examine image integrity here\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.float().unsqueeze(1).to(device=DEVICE)\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = net(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            # backward \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            print(\"end of epoch\")\n",
    "    print(\"~~~~ Out of train ~~~~\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1801e7-e791-46db-88b1-cdb77839d968",
   "metadata": {},
   "source": [
    "## Weighted Average Calculation for Aggregated Metrics\n",
    "\n",
    "This section defines the `weighted_average` function, a crucial component in federated learning for aggregating metrics from different clients. The function calculates a weighted average of various performance metrics across clients, considering the number of examples each client contributed. This approach ensures that the aggregated metrics accurately reflect the performance across the entire distributed dataset.\n",
    "\n",
    "### Functionality of `weighted_average`:\n",
    "1. **Purpose:** To compute the weighted average of various metrics (accuracy, dice score, IoU score, and loss) from multiple clients in a federated learning setup.\n",
    "2. **Parameters:** \n",
    "   - `metrics`: A list of tuples, where each tuple contains the number of examples used by a client and the metrics dictionary reported by that client.\n",
    "\n",
    "3. **Processing Steps:**\n",
    "   - **Metric Extraction and Weighting:** For each metric (accuracy, dice score, IoU score, loss), the function multiplies the metric value by the number of examples contributed by the client. This step ensures that clients with more data have a proportionally greater influence on the final averaged metric.\n",
    "   - **Aggregation:** The function then calculates the sum of these weighted metrics and divides it by the total number of examples across all clients. This results in a weighted average for each metric.\n",
    "\n",
    "4. **Return Value:**\n",
    "   - The function returns a dictionary containing the aggregated metrics: weighted average accuracy, dice score, IoU score, and loss.\n",
    "\n",
    "### Significance in Federated Learning:\n",
    "- In federated learning, where data is distributed across multiple clients, simple averaging might not accurately represent model performance due to the varying amount of data each client holds.\n",
    "- The `weighted_average` function addresses this by calculating weighted averages, providing a more representative measure of the model's overall performance across all clients.\n",
    "\n",
    "This function plays a critical role in evaluating the federated learning model, ensuring that the aggregated metrics are a fair representation of the model's performance on the diverse and distributed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7c525-8f1a-4c3a-854e-25895213b388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    print(\"_____metrics_______\")\n",
    "    print(metrics)\n",
    "    print(\">>>>>>>>>>>\")\n",
    "\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics] # Original\n",
    "    dice = [ num_examples *m[\"dice_score\"] for num_examples, m in metrics]\n",
    "    iouS = [num_examples * m[\"iouS\"] for num_examples, m in metrics]\n",
    "    loss = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
    "\n",
    "    #accuracies = [m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    acc = sum(accuracies)/sum(examples)\n",
    "    dice_score = sum(dice)/sum(examples)\n",
    "    iou_score = sum(iouS)/sum(examples)\n",
    "    loss = sum(loss)/sum(examples)\n",
    "\n",
    "    #wandb.log({\"acc\": acc, \"dice_score\": dice_score, \"iou_score\": iou_score, \"loss\": loss})\n",
    "\n",
    "    return {\"accuracies\": acc, \"dice_score\": dice_score, \"iouS\": iou_score, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c6724-9659-4765-a911-5b0b57879048",
   "metadata": {},
   "source": [
    "## Custom Federated Averaging Strategy with Model Saving\n",
    "\n",
    "In this part of the notebook, we define `SaveModelStrategy`, a custom class that extends Flower's federated averaging strategy (`FedAvg`). This class is tailored to not only aggregate model weights and metrics from different clients but also save the aggregated model state after each round of federated training.\n",
    "\n",
    "### Key Features of `SaveModelStrategy`:\n",
    "1. **Aggregate and Save Model Weights:**\n",
    "   - The `aggregate_fit` method is overridden to add functionality for saving the model.\n",
    "   - After aggregating parameters using the standard federated averaging method, the aggregated model weights are saved to a file. This is crucial for tracking the evolution of the model over training rounds and for potential later use or analysis.\n",
    "\n",
    "2. **Process of Aggregation and Saving:**\n",
    "   - **Parameter Aggregation:** Initially, the method calls its superclass (`FedAvg`) to perform the standard aggregation of parameters and metrics.\n",
    "   - **Model Saving:** If the aggregation is successful, the method converts the aggregated parameters into a format compatible with PyTorch's `state_dict`, and then updates the model's state.\n",
    "   - **Checkpoint Creation:** The model's state is saved as a checkpoint file, named according to the current training round. This enables checkpointing, allowing for model recovery and evaluation at different training stages.\n",
    "\n",
    "3. **Return Value:**\n",
    "   - The method returns the aggregated parameters and metrics, consistent with the expected output of an aggregation function in federated learning.\n",
    "\n",
    "### Importance in Federated Learning:\n",
    "- The ability to save model checkpoints during federated training is valuable for monitoring model progress, debugging, and potentially resuming training from a specific round.\n",
    "- This custom strategy enhances the basic federated averaging by adding a crucial aspect of model management, making it more practical for real-world federated learning scenarios.\n",
    "\n",
    "The `SaveModelStrategy` class provides a concrete example of how federated learning strategies can be extended and customized to meet specific requirements, such as model checkpointing in this case.\n",
    "\n",
    "##### *Define the path you want to save each round's aggregated model in this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ae5bc-6b97-4073-9998-becd41b488b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate model weights using weighted average and store checkpoint\"\"\"\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        if aggregated_parameters is not None:\n",
    "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Convert `List[np.ndarray]` to PyTorch`state_dict`\n",
    "            params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(net.state_dict(), f\"review_3c_12bs_000001_bce/model_round_{server_round}.pth\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8aa51-90fe-4adf-a9ae-2be734832afd",
   "metadata": {},
   "source": [
    "## Function for Saving Model Predictions as Images\n",
    "\n",
    "This section of the notebook introduces the `save_predictions_as_imgs` function. It is designed to save the model's predictions as image files, allowing for a visual inspection of the model's performance on a given dataset. This function is particularly useful in scenarios where evaluating the quality of segmentation or other image-based outputs is crucial.\n",
    "\n",
    "### Functionality of `save_predictions_as_imgs`:\n",
    "1. **Purpose:** To generate and save prediction images from the model for a given dataset. This visual output helps in assessing the model's accuracy and effectiveness in tasks like image segmentation.\n",
    "2. **Parameters:** \n",
    "   - `loader`: DataLoader containing the dataset to be processed.\n",
    "   - `model`: The trained model used for generating predictions.\n",
    "   - `client_id`: Identifier for the client (useful in federated learning contexts).\n",
    "   - `folder`: Destination folder for saving the images.\n",
    "   - `device`: The device (e.g., CPU, CUDA) on which the model is running.\n",
    "\n",
    "3. **Process:**\n",
    "   - The function iterates over the dataset provided by the loader.\n",
    "   - For each batch, it computes the model's predictions, applies a threshold to obtain binary output, and saves these predictions as images.\n",
    "   - Both the predictions and the corresponding ground truth labels (if available) are saved, allowing for a direct comparison.\n",
    "   - Images are saved with a naming convention that includes the batch index and the client identifier, ensuring easy tracking and organization.\n",
    "\n",
    "4. **Model State Management:**\n",
    "   - The model is set to evaluation mode (`model.eval()`) before processing to disable any training-specific behaviors like dropout.\n",
    "   - Once the predictions are saved, the model is reverted back to training mode (`model.train()`).\n",
    "\n",
    "### Practical Applications:\n",
    "- This function is invaluable for visually evaluating the model's performance, particularly in tasks where understanding the spatial accuracy of predictions (like segmentation) is essential.\n",
    "- In federated learning, where models are trained across different clients, this function aids in understanding the model's performance specific to each client's data.\n",
    "\n",
    "The `save_predictions_as_imgs` function exemplifies how model outputs can be materialized into a tangible format, providing a practical tool for visual analysis and presentation of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04408e0b-c617-440d-8e80-d86df69d5c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_predictions_as_imgs(\n",
    "    loader, model, client_id, folder=\"prediction_images/\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}_{client_id}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}_{client_id}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20e8ab-7c9f-4d02-929c-7a0e7184ff45",
   "metadata": {},
   "source": [
    "## Functions for Handling Model Parameters in Federated Learning\n",
    "\n",
    "In this section, we define two essential functions, `get_parameters` and `set_parameters`, which are crucial for managing the model's parameters in a federated learning context. These functions facilitate the extraction and update of model parameters, enabling the synchronization of local models with global parameters across different clients.\n",
    "\n",
    "### Function for Extracting Model Parameters (`get_parameters`):\n",
    "1. **Purpose:** Retrieves the parameters of a PyTorch model as a list of NumPy arrays. This is typically used to gather the local model parameters from a client in federated learning.\n",
    "2. **Parameters:** \n",
    "   - `net`: The neural network model from which parameters are to be extracted.\n",
    "3. **Functionality:** \n",
    "   - The function iterates over the model's state dictionary (`state_dict`) and converts each parameter tensor to a NumPy array.\n",
    "   - This conversion facilitates easy transmission of parameters over networks in federated learning scenarios.\n",
    "\n",
    "### Function for Updating Model with Global Parameters (`set_parameters`):\n",
    "1. **Purpose:** Updates a local model with a new set of parameters. This is typically used to synchronize a client's local model with the global model parameters in federated learning.\n",
    "2. **Parameters:** \n",
    "   - `device`: The device (e.g., CPU, GPU) on which the model is running.\n",
    "   - `net`: The neural network model to be updated.\n",
    "   - `parameters`: A list of NumPy arrays representing the new model parameters.\n",
    "3. **Process:**\n",
    "   - The function creates a new state dictionary for the model by mapping the provided parameters to the model's parameter keys.\n",
    "   - The model's state is then updated with this new state dictionary, effectively synchronizing it with the provided parameters.\n",
    "\n",
    "### Importance in Federated Learning:\n",
    "- These functions are fundamental in federated learning frameworks, where model parameters need to be frequently exchanged between the server and clients.\n",
    "- `get_parameters` allows for the efficient collection of local model parameters, while `set_parameters` ensures that local models are consistently updated with global advancements.\n",
    "\n",
    "Utilizing these functions, federated learning systems can maintain coherence and synchronization across distributed models, ensuring that all participating models are updated and trained in unison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02e89f-f6dd-4890-9d2e-835c60785b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get local params\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "#update local model with global params\n",
    "def set_parameters(device, net, parameters: List[np.ndarray]):\n",
    "    print(\"in outside SP !!!!!!!!\")\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485bb26-e2df-4b32-9d41-fb6a683e84d2",
   "metadata": {},
   "source": [
    "## Custom Flower Client for Federated Learning\n",
    "\n",
    "In this section, we introduce the `FlowerClient` class, a custom client implementation for federated learning using the Flower framework. This class is responsible for handling the training and evaluation of a neural network model (such as U-Net) on each client's local data. Additionally, we define the `client_fn` function, which instantiates and configures these clients.\n",
    "\n",
    "### Overview of `FlowerClient` Class:\n",
    "1. **Initialization:**\n",
    "   - The class is initialized with a client ID, neural network model (`net`), training and validation data loaders, device, learning rate, and the number of training epochs.\n",
    "2. **Parameter Synchronization Methods:**\n",
    "   - `set_parameters`: Updates the client's local model with global parameters received from the server.\n",
    "   - `get_parameters`: Retrieves the current parameters of the client's local model.\n",
    "3. **Training and Evaluation:**\n",
    "   - `fit`: Trains the local model on the client's data using the provided global parameters and returns the updated model parameters.\n",
    "   - `evaluate`: Evaluates the model on the client's validation data and returns the performance metrics.\n",
    "\n",
    "### Functionality of `client_fn`:\n",
    "- **Purpose:** This function is a factory for creating `FlowerClient` instances.\n",
    "- **Parameters:** It takes a client ID (`cid`) and uses it to set up a new client with its own model, training and validation data loaders, and training configurations.\n",
    "- **Implementation:** It initializes a U-Net model, assigns the respective data loaders for training and validation, and sets the learning rate and epochs for training.\n",
    "\n",
    "### Significance in Federated Learning:\n",
    "- The `FlowerClient` class encapsulates the behavior of a federated learning client, including how it trains locally and communicates with the federated server.\n",
    "- By using the `client_fn`, we can easily create multiple client instances, each with its unique dataset and model, facilitating the distributed training process in federated learning.\n",
    "\n",
    "This custom implementation demonstrates how federated learning clients can be tailored to specific requirements, such as training specialized models like U-Net, and how they can be efficiently managed and configured within the Flower framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57676f7d-10a0-48a5-bb34-f69c4a90a3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader, device, learning_rate, epochs):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        print(f\"[Client {self.cid}] set_parameters\")\n",
    "        # set_parameters(self.device, self.net, parameters)\n",
    "        params_dict = zip(self.net.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\n",
    "        #return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        self.set_parameters(parameters)\n",
    "        train(self.net, self.trainloader, self.epochs, self.lr, self.device)\n",
    "        return self.get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.set_parameters(parameters)\n",
    "        #loss, accuracy = test(self.net, self.valloader)\n",
    "        result = check_accuracy(self.valloader, self.net, self.device)\n",
    "        # print some examples to a folderthu\n",
    "        #save_predictions_as_imgs(self.valloader, self.net, self.cid, folder=\"prediction_images/\",device=self.device)\n",
    "        #[acc.item(), diceS.item(), iouS.item(), loss, correct_pixel.item(), total_pixel]\n",
    "        print('~~~ loss = ', str(result[3]))\n",
    "        return float(result[3]), len(self.valloader), {\"accuracy\": result[0], \"dice_score\": result[1], \"iouS\": result[2], \"loss\": result[3]}\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = UNET().to(DEVICE)\n",
    "    trainloader = train_loaders[int(cid)]\n",
    "    valloader = test_loaders[int(cid)]\n",
    "    lr = 0.000001\n",
    "    epochs = 30\n",
    "    print(\"~~~~client created~~~~\")\n",
    "    return FlowerClient(cid, net, trainloader, valloader, DEVICE, lr, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecc339-a20b-4d11-aa69-15bd666c2e7d",
   "metadata": {},
   "source": [
    "## Initializing the Federated Learning Environment with Flower\n",
    "\n",
    "This section of the notebook focuses on setting up the federated learning environment using Flower, a framework for building federated learning systems. The setup involves creating a model instance, defining a custom training strategy, and configuring the federated learning server and clients.\n",
    "\n",
    "### Model Initialization and Parameter Extraction:\n",
    "1. **Model Creation:** An instance of the U-Net model (`net`) is created and moved to the specified device (e.g., GPU or CPU).\n",
    "2. **Parameter Extraction:** The initial parameters of the model are extracted using the `get_parameters` function. These parameters serve as the starting point for federated learning.\n",
    "\n",
    "### Custom Strategy Definition (`SaveModelStrategy`):\n",
    "- A custom strategy called `SaveModelStrategy` is defined, which extends Flower's federated averaging strategy.\n",
    "- Key configurations of this strategy include:\n",
    "   - `fraction_fit` and `fraction_evaluate`: Proportions of clients that participate in training and evaluation.\n",
    "   - `min_fit_clients` and `min_evaluate_clients`: Minimum number of clients required for training and evaluation.\n",
    "   - `initial_parameters`: The initial model parameters, converted to Flower's parameter format.\n",
    "   - `evaluate_metrics_aggregation_fn`: Function to aggregate evaluation metrics, set to `weighted_average` for a representative aggregation.\n",
    "\n",
    "### Federated Learning Simulation Configuration:\n",
    "1. **Client Function:** The `client_fn` function is used to instantiate clients. Each client will have its instance of the U-Net model and its dataset.\n",
    "2. **Number of Clients:** The total number of clients participating in the federated learning is set to `NUM_CLIENTS`.\n",
    "3. **Server Configuration:** \n",
    "   - The server is configured to run a specified number of training rounds (`num_rounds`).\n",
    "4. **Client Resources:** \n",
    "   - If the model is trained on a GPU, client resources are specified accordingly (e.g., number of GPUs).\n",
    "5. **Starting the Simulation:** \n",
    "   - The simulation is started using `fl.simulation.start_simulation`, passing the client function, number of clients, server configuration, strategy, and client resources.\n",
    "\n",
    "### Importance of This Setup:\n",
    "- This setup is critical for launching a federated learning system where multiple clients train a shared model in a distributed manner.\n",
    "- It demonstrates how to initialize a federated learning environment, define a custom strategy for training and evaluation, and configure the server and clients for simulation.\n",
    "\n",
    "This initialization process sets the stage for federated training of the U-Net model, leveraging Flower's capabilities to manage and coordinate the distributed learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667dd31-e8f3-49e4-ba66-7c2c5593e2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = UNET().to(DEVICE)\n",
    "params = get_parameters(net)\n",
    "\n",
    "strategy = SaveModelStrategy(\n",
    "    fraction_fit=1.0,\n",
    "    fraction_evaluate=1.0,\n",
    "    min_fit_clients=3,\n",
    "    min_evaluate_clients=2,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 4}\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=300),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
